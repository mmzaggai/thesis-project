{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f89cc-b981-48b8-80a1-07a8e1ab9905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Documents to model: 50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 11:10:34,111 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 1563/1563 [15:30<00:00,  1.68it/s]\n",
      "2025-10-16 11:26:06,799 - BERTopic - Embedding - Completed ✓\n",
      "2025-10-16 11:26:06,799 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-10-16 11:26:44,269 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-16 11:26:44,282 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-10-16 11:27:30,659 - BERTopic - Cluster - Completed ✓\n",
      "2025-10-16 11:27:30,688 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-10-16 11:27:41,106 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] Fit completed in 17.2 minutes.\n",
      "[ok] per-document topics → topics_per_doc_mts50.csv  (rows: 50,000)\n",
      "[ok] topic summary → topic_summary_mts50.csv  (topics: 103)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>28287</td>\n",
       "      <td>-1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, that, is, in, you, it, they]</td>\n",
       "      <td>[the problem with blatant intellectual dishone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1518</td>\n",
       "      <td>0_land_the_jews_of</td>\n",
       "      <td>[land, the, jews, of, and, to, was, the land, ...</td>\n",
       "      <td>[i m genuinely trying to understand what you m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1018</td>\n",
       "      <td>1_hamas_hamas is_is_they</td>\n",
       "      <td>[hamas, hamas is, is, they, to, the, and, isra...</td>\n",
       "      <td>[as far as the reason this conflict can t end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>864</td>\n",
       "      <td>2_idf_the idf_the_idf is</td>\n",
       "      <td>[idf, the idf, the, idf is, they, is, and, to,...</td>\n",
       "      <td>[is that what the idf is doing in gaza, the id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>819</td>\n",
       "      <td>3_land_you_the_to</td>\n",
       "      <td>[land, you, the, to, it, of, that, and, is, pe...</td>\n",
       "      <td>[that means nothing all of those settlements a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>788</td>\n",
       "      <td>4_israel_us_the us_israel is</td>\n",
       "      <td>[israel, us, the us, israel is, the, is, to, i...</td>\n",
       "      <td>[those are all great questions reality is it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>691</td>\n",
       "      <td>5_genocide_is_the_of</td>\n",
       "      <td>[genocide, is, the, of, it, that, to, israel, ...</td>\n",
       "      <td>[you begin with a serious sounding disclaimer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>662</td>\n",
       "      <td>6_islam_muslims_muslim_religion</td>\n",
       "      <td>[islam, muslims, muslim, religion, the, and, i...</td>\n",
       "      <td>[wtf is this i wanna see those statistics prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>622</td>\n",
       "      <td>7_genocide_group_of_it</td>\n",
       "      <td>[genocide, group, of, it, the, of genocide, yo...</td>\n",
       "      <td>[genocide is the deliberate and systematic ext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>553</td>\n",
       "      <td>8_state_solution_state solution_palestinians</td>\n",
       "      <td>[state, solution, state solution, palestinians...</td>\n",
       "      <td>[because at the time when we are trying to ach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                          Name  \\\n",
       "0     -1  28287                              -1_the_and_to_of   \n",
       "1      0   1518                            0_land_the_jews_of   \n",
       "2      1   1018                      1_hamas_hamas is_is_they   \n",
       "3      2    864                      2_idf_the idf_the_idf is   \n",
       "4      3    819                             3_land_you_the_to   \n",
       "5      4    788                  4_israel_us_the us_israel is   \n",
       "6      5    691                          5_genocide_is_the_of   \n",
       "7      6    662               6_islam_muslims_muslim_religion   \n",
       "8      7    622                        7_genocide_group_of_it   \n",
       "9      8    553  8_state_solution_state solution_palestinians   \n",
       "\n",
       "                                      Representation  \\\n",
       "0    [the, and, to, of, that, is, in, you, it, they]   \n",
       "1  [land, the, jews, of, and, to, was, the land, ...   \n",
       "2  [hamas, hamas is, is, they, to, the, and, isra...   \n",
       "3  [idf, the idf, the, idf is, they, is, and, to,...   \n",
       "4  [land, you, the, to, it, of, that, and, is, pe...   \n",
       "5  [israel, us, the us, israel is, the, is, to, i...   \n",
       "6  [genocide, is, the, of, it, that, to, israel, ...   \n",
       "7  [islam, muslims, muslim, religion, the, and, i...   \n",
       "8  [genocide, group, of, it, the, of genocide, yo...   \n",
       "9  [state, solution, state solution, palestinians...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [the problem with blatant intellectual dishone...  \n",
       "1  [i m genuinely trying to understand what you m...  \n",
       "2  [as far as the reason this conflict can t end ...  \n",
       "3  [is that what the idf is doing in gaza, the id...  \n",
       "4  [that means nothing all of those settlements a...  \n",
       "5  [those are all great questions reality is it w...  \n",
       "6  [you begin with a serious sounding disclaimer ...  \n",
       "7  [wtf is this i wanna see those statistics prov...  \n",
       "8  [genocide is the deliberate and systematic ext...  \n",
       "9  [because at the time when we are trying to ach...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 11:27:46,870 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] saved model → bertopic_model_mts50.pkl\n",
      "[ok] topics overview HTML → topics_overview_mts50.html\n",
      "[ok] topic barchart HTML → topic_barchart_top20_mts50.html\n",
      "\n",
      "Next: map topic IDs to human-readable labels and assign stance (pro-Israel / pro-Palestine / neutral).\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 — BERTopic\n",
    "# Input : merged_preprocessed_for_topics.csv\n",
    "# Outputs: topics_per_doc.csv, topic_summary.csv\n",
    "# Test run (MTS= 10 / 30 / 50)\n",
    "# FINAL run (MTS=30)\n",
    "\n",
    "# %pip install -q bertopic sentence-transformers umap-learn hdbscan plotly\n",
    "\n",
    "# inspired by https://www.pinecone.io/learn/bertopic/\n",
    "# [https://www.kaggle.com/code/samvelkoch/tutorial-bertopic-best-practices](https://www.kaggle.com/code/samvelkoch/tutorial-bertopic-best-practices) \n",
    "# [https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html](https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html) \n",
    "# [https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Config \n",
    "INPUT_CSV        = \"merged_preprocessed_for_topics.csv\"   # from cleaning step\n",
    "TEXT_COL         = \"clean_for_topics\"                     # text column to model on\n",
    "ID_COLS_PREF     = [\"id\"]                                 # kept if present\n",
    "TIME_COL_PREFS   = [\"dt\", \"date\", \"created_utc\"]          # one will be kept if present\n",
    "\n",
    "# Sampling: start small for quick tests, then set to None to use all rows\n",
    "MAX_DOCS         = 50_000        # set to None to use ALL data\n",
    "RANDOM_STATE     = 42\n",
    "\n",
    "# BERTopic hyperparams\n",
    "EMBEDDING_MODEL  = \"all-MiniLM-L6-v2\"     \n",
    "MIN_TOPIC_SIZE   = 50                      # try 50, 30, 10 in separate runs\n",
    "N_GRAM_RANGE     = (1, 2)\n",
    "CALC_PROBS       = True\n",
    "\n",
    "# Output filenames\n",
    "TOPICS_PER_DOC_CSV = f\"topics_per_doc_mts{MIN_TOPIC_SIZE}.csv\"\n",
    "TOPIC_SUMMARY_CSV  = f\"topic_summary_mts{MIN_TOPIC_SIZE}.csv\"\n",
    "MODEL_PKL          = f\"bertopic_model_mts{MIN_TOPIC_SIZE}.pkl\"\n",
    "HTML_TOPICS_OVERV  = f\"topics_overview_mts{MIN_TOPIC_SIZE}.html\"\n",
    "HTML_BARCHART_TOPN = f\"topic_barchart_top20_mts{MIN_TOPIC_SIZE}.html\"\n",
    "\n",
    "# Load & prepare data\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected '{TEXT_COL}' in {INPUT_CSV}.\")\n",
    "\n",
    "# Keep optional metadata\n",
    "id_cols = [c for c in ID_COLS_PREF if c in df.columns]\n",
    "time_col = next((c for c in TIME_COL_PREFS if c in df.columns), None)\n",
    "\n",
    "# Make sure dt is proper datetime if present\n",
    "if \"dt\" in df.columns:\n",
    "    df[\"dt\"] = pd.to_datetime(df[\"dt\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Drop empty text \n",
    "df = df[df[TEXT_COL].astype(str).str.strip().astype(bool)].copy()\n",
    "if (MAX_DOCS is not None) and (len(df) > MAX_DOCS):\n",
    "    df = df.sample(MAX_DOCS, random_state=RANDOM_STATE).copy()\n",
    "\n",
    "docs = df[TEXT_COL].astype(str).tolist()\n",
    "print(f\"[info] Documents to model: {len(df):,}\")\n",
    "\n",
    "# Fit BERTopic\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedder,\n",
    "    language=\"english\",\n",
    "    min_topic_size=MIN_TOPIC_SIZE,\n",
    "    n_gram_range=N_GRAM_RANGE,\n",
    "    calculate_probabilities=CALC_PROBS,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "mins = (time.time() - t0) / 60.0\n",
    "print(f\"[done] Fit completed in {mins:.1f} minutes.\")\n",
    "\n",
    "# Save per-document topics\n",
    "out = pd.DataFrame({\"topic_id\": topics})\n",
    "if CALC_PROBS and probs is not None:\n",
    "    out[\"topic_prob\"] = np.max(probs, axis=1)\n",
    "else:\n",
    "    out[\"topic_prob\"] = None\n",
    "\n",
    "# Bring back metadata (if available)\n",
    "if id_cols:\n",
    "    for c in id_cols:\n",
    "        out[c] = df[c].values\n",
    "if time_col and time_col in df.columns:\n",
    "    out[time_col] = df[time_col].values\n",
    "if \"subreddit\" in df.columns:\n",
    "    out[\"subreddit\"] = df[\"subreddit\"].values\n",
    "if \"kind\" in df.columns:\n",
    "    out[\"kind\"] = df[\"kind\"].values\n",
    "\n",
    "# Keep raw & clean text for later reference\n",
    "keep_text_cols = []\n",
    "if \"text\" in df.columns and \"text\" != TEXT_COL:\n",
    "    keep_text_cols.append(\"text\")\n",
    "keep_text_cols.append(TEXT_COL)\n",
    "for c in keep_text_cols:\n",
    "    if c in df.columns:\n",
    "        out[c] = df[c].values\n",
    "\n",
    "# Nice ordering\n",
    "preferred = id_cols + ([\"subreddit\",\"kind\"] if \"subreddit\" in out.columns else []) \\\n",
    "            + ([time_col] if time_col and time_col in out.columns else []) \\\n",
    "            + keep_text_cols + [\"topic_id\",\"topic_prob\"]\n",
    "ordered_cols = [c for c in preferred if c in out.columns] + \\\n",
    "               [c for c in out.columns if c not in preferred]\n",
    "out = out[ordered_cols]\n",
    "\n",
    "out.to_csv(TOPICS_PER_DOC_CSV, index=False)\n",
    "print(f\"[ok] per-document topics → {TOPICS_PER_DOC_CSV}  (rows: {len(out):,})\")\n",
    "\n",
    "# Save topic summary\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.to_csv(TOPIC_SUMMARY_CSV, index=False)\n",
    "print(f\"[ok] topic summary → {TOPIC_SUMMARY_CSV}  (topics: {len(topic_info):,})\")\n",
    "display(topic_info.head(10))\n",
    "\n",
    "\n",
    "# Save model as a single file\n",
    "topic_model.save(MODEL_PKL)  # default serialization=\"pickle\"\n",
    "print(f\"[ok] saved model → {MODEL_PKL}\")\n",
    "\n",
    "\n",
    "# HTML visuals\n",
    "try:\n",
    "    fig_over = topic_model.visualize_topics()\n",
    "    fig_over.write_html(HTML_TOPICS_OVERV)\n",
    "    print(f\"[ok] topics overview HTML → {HTML_TOPICS_OVERV}\")\n",
    "\n",
    "    # Top 20 topics by size\n",
    "    top_topic_ids = topic_info[topic_info.Topic != -1].sort_values(\"Count\", ascending=False)[\"Topic\"].head(20).tolist()\n",
    "    fig_bar = topic_model.visualize_barchart(topics=top_topic_ids)\n",
    "    fig_bar.write_html(HTML_BARCHART_TOPN)\n",
    "    print(f\"[ok] topic barchart HTML → {HTML_BARCHART_TOPN}\")\n",
    "except Exception as e:\n",
    "    print(f\"[warn] visualization skipped: {e}\")\n",
    "\n",
    "print(\"\\nNext: map topic IDs to human-readable labels and assign stance (pro-Israel / pro-Palestine / neutral).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
