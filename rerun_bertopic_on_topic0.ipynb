{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f98cb-c6eb-4cd1-bf94-ee8c00bfd0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading file: topics_per_doc_mts30_fast.csv\n",
      "[info] Extracted 114,522 rows with topic == 0\n",
      "[ok] Saved Topic 0 data → C:\\Users\\Matilde\\Desktop\\thesis\\Thesis_project_2.0\\topic_0.csv\n",
      "\n",
      "Preview:\n",
      "    id       subreddit kind                        dt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      clean_for_topics  topic                       topic_name\n",
      "gnle2w IsraelPalestine post 2020-05-20 23:18:00+00:00 can we stop blaming israel for kids hamas kills original title was can we please stop blaming israel for the dead children that hamas puts in the line of fire but theres a character limit so i had to butcher it a little x b its very disturbing to me that when you talk to a lot of the anti zionist crowd about the conflict they keep talking about children killed yes a ridiculous number of children are killed in this conflict yet israel goes to equally ridiculous lengths to minimize civilian casualties the problem is hamas is doing everything they can to maximize these casualties especially children all to pit people against israel by blaming israel for the actions of hamas you are essentially letting them get away with it x b it is a fact firstly that hamas launches mortars from palestinian civilian areas at israeli civilian areas israels and any governments first responsibility is to keep its citizens safe so they respond with a surgical strike hamas bug out and ensure there are civilians left to die theres even evidence of them setting up explosives around the area that will be triggered by israels surgical strike making a far bigger explosion and hurting far more people than the idf couldve possibly known x b secondly their insistence of using human shields and yes ive seen the videos on youtube of idf soldiers supposedly doing the same thing bringing a translator with you or detaining somebody in a car is not a human shield x b and probably most disturbing of all is the brainwashing these kids receive at school and at home all you need do is search youtube for palestinian childrens shows which imho is the most fucked up shit ive ever seen youve got little year old girls talking about how much they hate jews and how they want nothing more than to be martyred they are literally raising kids to believe the greatest thing they can accomplish in life the most noble thing they can do is to die whilst killing jews x b i m sorry but how the fuck is israel or anyone meant to prevent children dying in that situation it s damn near impossible yes sure theres innocent bystanders accidentally shot happens in any armed conflict however the fact that theres so goddamn many can be attributed directly to the actions of hamas and i havent even talked about their penchant for storing explosives in un run schools or firing mortars from hospitals im only covering the bare basics here x b the idf an army made up mostly of conscripted teenagers doesnt wanna kill children its not something they set out to do and even if you believe they dont care about palestinian children for whatever reason the pr alone is nightmare oh im sure theres the odd sociopath in there who might enjoy killing but most of the soldiers and former soldiers ive met are very well grounded people who dont get off on killing the idf even allows their soldiers to disobey direct orders if they believe them to be morally wrong i know of no other military that allows this this is because of the we were just following orders defense at the nuremberg trials the israeli govt wanted to ensure that if a soldier got an order they thought was wrong like blow up that school full of kids they could just say no yet hamas leadership have no trouble using this tactic the more children they put in harms way the better they look and theyll continue using this disgusting tactic as long as people blame israel for the deaths the best way to stop it put the blame where it belongs on hamas by blaming israel you allow this to continue and you ensure that there will be more children dead x b most palestinians and arabs in general know hamas doesnt give a shit about palestinians if you honestly think they do then youre just as brainwashed as those poor kids who grow up with dreams of dying for a bullshit cause yet the antizionist lot tend to argue that theyre just freedom fighters doing what they have to sorry but no freedom fighters try to preserve the life of their people especially their children they dont put them in harms way while they run and hide thats what a terrorist does x b thats it my rant is over this part of the conflict really pisses me off because its so obvious the info is there but when you bring it up youre accused of being hasbara no real rebuttal just accusations of lies and propaganda      0 0_genocide_comments_israel_hamas\n",
      "lb9cr8 IsraelPalestine post 2021-02-02 23:08:05+00:00                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              the israel iran conflict      0 0_genocide_comments_israel_hamas\n",
      "mlaert IsraelPalestine post 2021-04-06 12:23:37+00:00                                                                                                                    bds founder in his own words please see some quotes below from the founder and leader of bds definitely most definitely we oppose a jewish state in any part of palestine no palestinian rational palestinian not a sell out palestinian would ever accept a jewish state in palestine omar barghouti may palestinians have a right to resistance by any means including armed resistance jews aren t indigenous just because you say you are jews are not a people the un s principle of the right to self determination applies only to colonized people who want to acquire their rights omar barghouti going back to the two state solution besides having passed its expiry date it was never a moral solution to start with we are witnessing the rapid demise of zionism and nothing can be done to save it for zionism is intent on killing itself omar bargouti good riddance the two state solution for the palestinian israeli conflict is finally dead but someone has to issue an official death certificate before the rotting corpse is given a proper burial and we can all move on and explore the more just moral and therefore enduring alternative for peaceful coexistence between jews and arabs in mandate palestine the one state solution omar bargouti the one state solution means a unitary state where by definition jews will be a minority omar bargouti i am completely and categorically against binationalism because it assumes that there are two nations with equal moral claims to the land omar bargouti if the refugees were to return you would not have a two state solution you d have a palestine next to a palestine omar barghouti quoting sari nusseibeh president of al quds university you can also hear him say this around of this video the two state solution as dictated by israel omits basic palestinian rights and would be yet another act of british complicity in bestowing legitimacy on israel s unjust order omar barghouti a jewish state in palestine in any shape or form cannot but contravene the basic rights of the indigenous palestinian population and perpetuate a system of racial discrimination that ought to be opposed categorically definitely most definitely we oppose a jewish state in any part of palestine no palestinian rational palestinian not a sell out palestinian will ever accept a jewish state in palestine omar barghouti you cannot reconcile the right of return for refugees with a two state solution a return for refugees would end israel s existence as a jewish state a two state solution was never moral and it s no longer working omar barghouti many of the methods of collective and individual punishment meted out to palestinian civilians at the hands of young racist often sadistic and ever impervious israeli soldiers at the hundreds of checkpoints littering the occupied palestinian territories are reminiscent of common nazi practices against the jews omar barghouti jews did not suffer in arab countries there were no pogroms there was no persecution omar barghouti read that last one the founder of bds openly denies the persecution of jews in arab countries you could stop there he is an antisemite there are quite a few other quotes from prominent bds leaders read this link for more barghouti doesn t just support the one state solution he supports the total delegitimization and disconnection of jews from their homeland he is pushing a lie that jews have absolutely no connection to the land and he is skewing history by saying that palestine came first barghouti doesn t want peace between israelis and palestinians he wants palestinian domination for the tables to turn for jews to be treated as outsiders in these quotes it is clear that barghouti is less concerned with helping palestinians than he is in forbidding any idea of there being a jewish state in the second quote from the top barghouti openly says that violence is ok in the form of resistance meaning he stands behind actual terror as well there is so much wrong with this unholy excuse for a human being pardon my french he thrives on violence it s the only way to stir up more supporters bds is not a movement for peace change my mind surely there are better ways to liberate palestinians than this      0 0_genocide_comments_israel_hamas\n",
      "na4hkg IsraelPalestine post 2021-05-11 18:52:42+00:00                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          no child should be scared they won t see tomorrow on both sides whether you are palestinian israeli both or neither i wish you a night of rest we don t deserve this our leaders have failed us time and time again they promised solutions but they don t walk the walk i wish every child every mother every soldier coming home to find their home just as they left it to never wake up to a broken household no child should live in fear of their school being blown to pieces i genuinely wish all of us citizens of the world however cliche it sounds a happy rocket less night sleep also sorry for the bad grammar in the title there s a characters limit so i had to make a cut      0 0_genocide_comments_israel_hamas\n",
      "nm40u4 IsraelPalestine post 2021-05-27 09:59:57+00:00                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                people are accusing israel of genocide human rights lawyers who have been deeply critical of israeli policy and actions say the term does not apply calling the accusation ridiculous accusations that israel has been committing genocide in gaza have flown freely from seasoned activists to the palestinian foreign minister to people wading into the israeli palestinian issue for the first time on social media tweets with the words israel and genocide appeared as often as thousands of times per hour on twitter during the day conflict in which more than palestinians and a dozen israelis were killed but even human rights lawyers who have been deeply critical of israeli policy and actions in gaza and the west bank say the genocide term doesn t apply first and foremost in order to commit the crime of genocide one needs to have an intention to exterminate in whole or in part a group sfard said and in the years of my activism and more than years of litigation i haven t seen a shred of evidence that israeli officials and decision makers hold such an intention      0 0_genocide_comments_israel_hamas\n"
     ]
    }
   ],
   "source": [
    "# STEP 9 — Zoom in on Topic 0 (split into subtopics)\n",
    "\n",
    "#Extract all documents assigned to topic 0 from topics_per_doc_mts30_fast.csv and save them into a separate CSV file.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Input / Output paths\n",
    "INPUT_FILE = Path(\"topics_per_doc_mts30_fast.csv\")\n",
    "OUTPUT_FILE = Path(\"topic_0.csv\")\n",
    "\n",
    "def main():\n",
    "    # 1. Load the full per-doc topics file\n",
    "    print(f\"[info] Loading file: {INPUT_FILE}\")\n",
    "    df = pd.read_csv(INPUT_FILE, low_memory=False)\n",
    "\n",
    "    # 2. Check for the topic column\n",
    "    if \"topic\" not in df.columns:\n",
    "        raise ValueError(\"Column 'topic' not found in input file!\")\n",
    "\n",
    "    # 3. Filter for topic == 0\n",
    "    df_topic0 = df.loc[df[\"topic\"] == 0].copy()\n",
    "    n_rows = len(df_topic0)\n",
    "    print(f\"[info] Extracted {n_rows:,} rows with topic == 0\")\n",
    "\n",
    "    if n_rows == 0:\n",
    "        print(\"[warn] No rows found for topic == 0. Check topic IDs in your data.\")\n",
    "        return\n",
    "\n",
    "    # 4. Save to new file\n",
    "    df_topic0.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"[ok] Saved Topic 0 data → {OUTPUT_FILE.resolve()}\")\n",
    "\n",
    "    # 5. Quick preview\n",
    "    print(\"\\nPreview:\")\n",
    "    print(df_topic0.head(5).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loaded 114,522 documents for Topic 0 re-clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 14:18:02,484 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Training subset: 20,000 docs (of 114,522)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 625/625 [06:27<00:00,  1.61it/s]\n",
      "2025-10-27 14:24:30,666 - BERTopic - Embedding - Completed ✓\n",
      "2025-10-27 14:24:30,666 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-10-27 14:24:52,389 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:24:52,391 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-10-27 14:24:58,808 - BERTopic - Cluster - Completed ✓\n",
      "2025-10-27 14:24:58,810 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-10-27 14:25:01,844 - BERTopic - Representation - Completed ✓\n",
      "2025-10-27 14:25:01,845 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-10-27 14:25:01,910 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-10-27 14:25:04,775 - BERTopic - Representation - Completed ✓\n",
      "2025-10-27 14:25:04,779 - BERTopic - Topic reduction - Reduced number of topics from 130 to 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Fit complete on training subset.\n",
      "[ok] Wrote topic summary → topic0_summary_mts30_topic0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 64/64 [00:57<00:00,  1.11it/s]\n",
      "2025-10-27 14:26:02,886 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:26:08,281 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:26:08,282 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:26:08,423 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:40<00:00,  1.60it/s]3s/it]\n",
      "2025-10-27 14:26:48,542 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:26:49,665 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:26:49,666 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:26:49,814 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:40<00:00,  1.57it/s]4s/it]\n",
      "2025-10-27 14:27:30,617 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:27:31,685 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:27:31,686 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:27:31,822 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:41<00:00,  1.53it/s]4s/it]\n",
      "2025-10-27 14:28:13,681 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:28:14,796 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:28:14,797 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:28:14,932 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:37<00:00,  1.70it/s]8s/it]\n",
      "2025-10-27 14:28:52,673 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:28:53,768 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:28:53,768 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:28:53,907 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s]5s/it]\n",
      "2025-10-27 14:29:27,233 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:29:28,335 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:29:28,336 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:29:28,469 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:33<00:00,  1.88it/s]0s/it]\n",
      "2025-10-27 14:30:02,504 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:30:03,584 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:30:03,584 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:30:03,721 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s]1s/it]\n",
      "2025-10-27 14:30:37,129 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:30:38,242 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:30:38,242 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:30:38,379 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:33<00:00,  1.92it/s]8s/it]\n",
      "2025-10-27 14:31:11,723 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:31:12,844 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:31:12,846 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:31:12,991 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:35<00:00,  1.81it/s]5s/it]\n",
      "2025-10-27 14:31:48,418 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:31:49,558 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:31:49,558 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:31:49,701 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:35<00:00,  1.82it/s]53s/it]\n",
      "2025-10-27 14:32:24,988 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:32:26,659 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:32:26,659 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:32:26,888 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:36<00:00,  1.76it/s]73s/it]\n",
      "2025-10-27 14:33:03,321 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:33:04,394 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:33:04,395 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:33:04,529 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:38<00:00,  1.68it/s]01s/it]\n",
      "2025-10-27 14:33:42,741 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:33:43,827 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:33:43,828 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:33:43,965 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:37<00:00,  1.71it/s]74s/it]\n",
      "2025-10-27 14:34:21,456 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:34:22,536 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:34:22,537 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:34:22,685 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:37<00:00,  1.70it/s]04s/it]\n",
      "2025-10-27 14:35:00,306 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:35:01,390 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:35:01,391 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:35:01,534 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:39<00:00,  1.61it/s]28s/it]\n",
      "2025-10-27 14:35:41,248 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:35:42,342 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:35:42,343 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:35:42,482 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:43<00:00,  1.47it/s]09s/it]\n",
      "2025-10-27 14:36:26,116 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:36:27,181 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:36:27,181 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:36:27,320 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:38<00:00,  1.68it/s]81s/it]\n",
      "2025-10-27 14:37:05,575 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:37:06,668 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:37:06,669 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:37:06,885 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:42<00:00,  1.50it/s]44s/it]\n",
      "2025-10-27 14:37:49,567 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:37:50,690 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:37:50,691 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:37:50,821 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:42<00:00,  1.50it/s]49s/it]\n",
      "2025-10-27 14:38:33,437 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:38:34,560 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:38:34,561 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:38:34,706 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:45<00:00,  1.40it/s]21s/it]\n",
      "2025-10-27 14:39:20,508 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:39:21,615 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:39:21,616 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:39:21,757 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:41<00:00,  1.54it/s]66s/it]\n",
      "2025-10-27 14:40:03,367 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:40:04,454 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:40:04,455 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:40:04,599 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:44<00:00,  1.43it/s]42s/it]\n",
      "2025-10-27 14:40:49,520 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:40:50,609 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:40:50,610 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:40:50,749 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:42<00:00,  1.51it/s]24s/it]\n",
      "2025-10-27 14:41:33,127 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:41:34,229 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:41:34,230 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:41:34,370 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:43<00:00,  1.46it/s]05s/it]\n",
      "2025-10-27 14:42:18,326 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:42:19,434 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:42:19,435 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:42:19,578 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:45<00:00,  1.40it/s]40s/it]\n",
      "2025-10-27 14:43:05,405 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:43:06,512 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:43:06,512 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:43:06,652 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:40<00:00,  1.58it/s]20s/it]\n",
      "2025-10-27 14:43:47,362 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:43:48,468 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:43:48,468 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:43:48,605 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:42<00:00,  1.52it/s]23s/it]\n",
      "2025-10-27 14:44:30,947 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:44:32,114 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:44:32,115 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:44:32,260 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:37<00:00,  1.70it/s]06s/it]\n",
      "2025-10-27 14:45:10,075 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:45:11,246 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:45:11,246 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:45:11,395 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:44<00:00,  1.44it/s]58s/it]\n",
      "2025-10-27 14:45:55,981 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:45:57,086 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:45:57,086 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:45:57,225 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:43<00:00,  1.47it/s]55s/it]\n",
      "2025-10-27 14:46:40,882 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:46:42,023 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:46:42,024 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:46:42,157 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:43<00:00,  1.46it/s]97s/it]\n",
      "2025-10-27 14:47:26,056 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:47:27,135 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:47:27,136 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:47:27,275 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:42<00:00,  1.50it/s]31s/it]\n",
      "2025-10-27 14:48:10,051 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:48:11,126 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:48:11,126 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:48:11,266 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:43<00:00,  1.48it/s]22s/it]\n",
      "2025-10-27 14:48:54,526 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:48:55,943 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:48:55,944 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:48:56,147 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:44<00:00,  1.45it/s]42s/it]\n",
      "2025-10-27 14:49:40,559 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:49:41,708 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:49:41,709 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:49:41,852 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:43<00:00,  1.46it/s]80s/it]\n",
      "2025-10-27 14:50:25,923 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:50:27,276 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:50:27,277 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:50:27,425 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:39<00:00,  1.60it/s]03s/it]\n",
      "2025-10-27 14:51:07,465 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:51:08,563 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:51:08,563 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:51:08,707 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:43<00:00,  1.46it/s]91s/it]\n",
      "2025-10-27 14:51:52,756 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:51:53,862 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:51:53,863 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:51:53,997 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:44<00:00,  1.43it/s]32s/it]\n",
      "2025-10-27 14:52:38,778 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:52:39,885 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:52:39,886 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:52:40,038 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:40<00:00,  1.56it/s]84s/it]\n",
      "2025-10-27 14:53:21,109 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:53:22,197 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:53:22,198 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:53:22,341 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:41<00:00,  1.56it/s]08s/it]\n",
      "2025-10-27 14:54:03,568 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:54:04,672 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:54:04,672 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:54:04,813 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:42<00:00,  1.51it/s]60s/it]\n",
      "2025-10-27 14:54:47,255 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:54:48,334 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:54:48,335 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:54:48,469 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:45<00:00,  1.40it/s]61s/it]\n",
      "2025-10-27 14:55:34,182 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:55:35,375 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:55:35,376 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:55:35,513 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:40<00:00,  1.58it/s]64s/it]\n",
      "2025-10-27 14:56:16,037 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:56:17,146 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:56:17,146 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:56:17,290 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:44<00:00,  1.43it/s]78s/it]\n",
      "2025-10-27 14:57:02,068 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:57:03,147 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:57:03,148 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:57:03,289 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:42<00:00,  1.52it/s]45s/it]\n",
      "2025-10-27 14:57:45,535 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:57:46,647 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:57:46,648 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:57:46,798 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:44<00:00,  1.44it/s]17s/it]\n",
      "2025-10-27 14:58:31,483 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:58:32,585 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:58:32,586 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:58:32,729 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:47<00:00,  1.35it/s]70s/it]\n",
      "2025-10-27 14:59:20,158 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 14:59:21,377 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 14:59:21,377 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 14:59:21,524 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:47<00:00,  1.34it/s]93s/it]\n",
      "2025-10-27 15:00:09,439 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 15:00:10,763 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 15:00:10,764 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 15:00:10,905 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:46<00:00,  1.37it/s]96s/it]\n",
      "2025-10-27 15:00:57,660 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 15:00:58,748 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 15:00:58,749 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 15:00:58,887 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:41<00:00,  1.54it/s]27s/it]\n",
      "2025-10-27 15:01:40,565 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 15:01:41,674 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 15:01:41,674 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 15:01:41,812 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s]97s/it]\n",
      "2025-10-27 15:02:12,818 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 15:02:13,879 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 15:02:13,880 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 15:02:14,014 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:29<00:00,  2.16it/s]84s/it]\n",
      "2025-10-27 15:02:43,665 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 15:02:44,752 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 15:02:44,753 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 15:02:44,890 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:30<00:00,  2.07it/s]55s/it]\n",
      "2025-10-27 15:03:15,831 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 15:03:16,940 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 15:03:16,940 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 15:03:17,087 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 64/64 [00:28<00:00,  2.28it/s]64s/it]\n",
      "2025-10-27 15:03:45,194 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 15:03:46,745 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 15:03:46,745 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 15:03:46,894 - BERTopic - Cluster - Completed ✓\n",
      "Batches: 100%|██████████| 59/59 [00:28<00:00,  2.07it/s]59s/it]\n",
      "2025-10-27 15:04:15,409 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-10-27 15:04:16,472 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-27 15:04:16,472 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-10-27 15:04:16,598 - BERTopic - Cluster - Completed ✓\n",
      "Transform full: 100%|██████████| 56/56 [39:11<00:00, 41.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Wrote per-doc topics → topic0_per_doc_mts30_topic0.csv\n",
      "topic\n",
      "-1    12232\n",
      " 0    72234\n",
      " 1     3496\n",
      " 2     3529\n",
      " 3     3305\n",
      " 4     2856\n",
      " 5     1128\n",
      " 6     1101\n",
      " 7      989\n",
      " 9      690\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>kind</th>\n",
       "      <th>dt</th>\n",
       "      <th>clean_for_topics</th>\n",
       "      <th>parent_topic</th>\n",
       "      <th>parent_topic_name</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gnle2w</td>\n",
       "      <td>IsraelPalestine</td>\n",
       "      <td>post</td>\n",
       "      <td>2020-05-20 23:18:00+00:00</td>\n",
       "      <td>can we stop blaming israel for kids hamas kill...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_genocide_comments_israel_hamas</td>\n",
       "      <td>0</td>\n",
       "      <td>0_hamas_gaza_genocide_palestinians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lb9cr8</td>\n",
       "      <td>IsraelPalestine</td>\n",
       "      <td>post</td>\n",
       "      <td>2021-02-02 23:08:05+00:00</td>\n",
       "      <td>the israel iran conflict</td>\n",
       "      <td>0</td>\n",
       "      <td>0_genocide_comments_israel_hamas</td>\n",
       "      <td>0</td>\n",
       "      <td>0_hamas_gaza_genocide_palestinians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mlaert</td>\n",
       "      <td>IsraelPalestine</td>\n",
       "      <td>post</td>\n",
       "      <td>2021-04-06 12:23:37+00:00</td>\n",
       "      <td>bds founder in his own words please see some q...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_genocide_comments_israel_hamas</td>\n",
       "      <td>0</td>\n",
       "      <td>0_hamas_gaza_genocide_palestinians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>na4hkg</td>\n",
       "      <td>IsraelPalestine</td>\n",
       "      <td>post</td>\n",
       "      <td>2021-05-11 18:52:42+00:00</td>\n",
       "      <td>no child should be scared they won t see tomor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_genocide_comments_israel_hamas</td>\n",
       "      <td>0</td>\n",
       "      <td>0_hamas_gaza_genocide_palestinians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nm40u4</td>\n",
       "      <td>IsraelPalestine</td>\n",
       "      <td>post</td>\n",
       "      <td>2021-05-27 09:59:57+00:00</td>\n",
       "      <td>people are accusing israel of genocide human r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_genocide_comments_israel_hamas</td>\n",
       "      <td>0</td>\n",
       "      <td>0_hamas_gaza_genocide_palestinians</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id        subreddit  kind                        dt  \\\n",
       "0  gnle2w  IsraelPalestine  post 2020-05-20 23:18:00+00:00   \n",
       "1  lb9cr8  IsraelPalestine  post 2021-02-02 23:08:05+00:00   \n",
       "2  mlaert  IsraelPalestine  post 2021-04-06 12:23:37+00:00   \n",
       "3  na4hkg  IsraelPalestine  post 2021-05-11 18:52:42+00:00   \n",
       "4  nm40u4  IsraelPalestine  post 2021-05-27 09:59:57+00:00   \n",
       "\n",
       "                                    clean_for_topics  parent_topic  \\\n",
       "0  can we stop blaming israel for kids hamas kill...             0   \n",
       "1                           the israel iran conflict             0   \n",
       "2  bds founder in his own words please see some q...             0   \n",
       "3  no child should be scared they won t see tomor...             0   \n",
       "4  people are accusing israel of genocide human r...             0   \n",
       "\n",
       "                  parent_topic_name  topic                          topic_name  \n",
       "0  0_genocide_comments_israel_hamas      0  0_hamas_gaza_genocide_palestinians  \n",
       "1  0_genocide_comments_israel_hamas      0  0_hamas_gaza_genocide_palestinians  \n",
       "2  0_genocide_comments_israel_hamas      0  0_hamas_gaza_genocide_palestinians  \n",
       "3  0_genocide_comments_israel_hamas      0  0_hamas_gaza_genocide_palestinians  \n",
       "4  0_genocide_comments_israel_hamas      0  0_hamas_gaza_genocide_palestinians  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-run BERTopic on Topic 0 data (zoom-in)\n",
    "# Input  : topic_0.csv  (subset of all docs originally assigned to topic 0)\n",
    "# Outputs: topic0_per_doc_mts30_topic0.csv, topic0_summary_mts30_topic0.csv\n",
    "\n",
    "# Config & imports\n",
    "import os, math\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Files\n",
    "INPUT = \"topic_0.csv\"                 # extracted Topic 0 file\n",
    "RUN_TAG = \"mts30_topic0\"              # unique tag so outputs don’t collide with the global run\n",
    "TOPICS_PER_DOC_CSV = f\"topic0_per_doc_{RUN_TAG}.csv\"\n",
    "TOPIC_SUMMARY_CSV  = f\"topic0_summary_{RUN_TAG}.csv\"\n",
    "\n",
    "# Hyperparams\n",
    "MIN_TOPIC_SIZE = 30\n",
    "TRAIN_MAX_DOCS = 20_000\n",
    "CAP_PER_WEEK   = 500\n",
    "BATCH_SIZE     = 2048\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\" \n",
    "\n",
    "# Speed knobs for BERTopic\n",
    "# Light embedding model\n",
    "embedder = SentenceTransformer(EMB_MODEL_NAME)\n",
    "\n",
    "#Fast UMAP (lower n_components, mild neighbors)\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=10,\n",
    "    n_components=3,\n",
    "    min_dist=0.2,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    low_memory=True,\n",
    ")\n",
    "\n",
    "# HDBSCAN tuned for speed/stability\n",
    "cluster_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=MIN_TOPIC_SIZE,\n",
    "    min_samples=max(5, MIN_TOPIC_SIZE // 2),\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data= True,  # speeds up a lot\n",
    ")\n",
    "\n",
    "# Vectorizer: cap vocab (min_df filters rares, max_features caps size)\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    max_features=60_000,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "# Create the BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedder,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=cluster_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    min_topic_size=MIN_TOPIC_SIZE,\n",
    "    nr_topics=\"auto\",\n",
    "    calculate_probabilities=False,\n",
    "    low_memory=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Load and prepare data\n",
    "df_full = pd.read_csv(INPUT, low_memory=False)\n",
    "assert \"clean_for_topics\" in df_full.columns, \"Missing 'clean_for_topics' column from preprocessing step.\"\n",
    "assert \"dt\" in df_full.columns, \"Missing 'dt' column.\"\n",
    "\n",
    "# Preserve parent topic information before overwriting it later\n",
    "if \"topic\" in df_full.columns:\n",
    "    df_full = df_full.rename(columns={\"topic\": \"parent_topic\", \"topic_name\": \"parent_topic_name\"})\n",
    "\n",
    "# Clean and filter text\n",
    "df_full[\"clean_for_topics\"] = df_full[\"clean_for_topics\"].astype(str).str.strip()\n",
    "df_full = df_full[df_full[\"clean_for_topics\"].str.split().str.len() >= 3].copy()\n",
    "df_full[\"dt\"] = pd.to_datetime(df_full[\"dt\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "print(f\"[info] Loaded {len(df_full):,} documents for Topic 0 re-clustering\")\n",
    "\n",
    "# Build stratified training subset (balanced in time & subreddit/kind)\n",
    "df_full[\"week\"] = df_full[\"dt\"].dt.to_period(\"W-MON\").astype(str)\n",
    "group_cols = [c for c in [\"subreddit\", \"kind\", \"week\"] if c in df_full.columns]\n",
    "if not group_cols:\n",
    "    group_cols = [\"week\"]\n",
    "\n",
    "def cap_group(g, cap=CAP_PER_WEEK):\n",
    "    if len(g) <= cap:\n",
    "        return g\n",
    "    return g.sample(cap, random_state=42)\n",
    "\n",
    "df_train = (\n",
    "    df_full.groupby(group_cols, group_keys=False)\n",
    "           .apply(cap_group, cap=CAP_PER_WEEK)\n",
    "           .sample(min(TRAIN_MAX_DOCS, len(df_full)), random_state=42)\n",
    "           .reset_index(drop=True)\n",
    ")\n",
    "print(f\"[info] Training subset: {len(df_train):,} docs (of {len(df_full):,})\")\n",
    "\n",
    "# Fit on subset\n",
    "train_texts = df_train[\"clean_for_topics\"].tolist()\n",
    "topics_train, _ = topic_model.fit_transform(train_texts)\n",
    "print(\"[ok] Fit complete on training subset.\")\n",
    "\n",
    "# Build & save topic summary\n",
    "topic_info = topic_model.get_topic_info().rename(\n",
    "    columns={\"Count\": \"count\", \"Name\": \"name\", \"Topic\": \"topic\"}\n",
    ")\n",
    "topic_info.to_csv(TOPIC_SUMMARY_CSV, index=False)\n",
    "print(f\"[ok] Wrote topic summary → {TOPIC_SUMMARY_CSV}\")\n",
    "\n",
    "\n",
    "# Transform the full dataset in batches (assign topics)\n",
    "\n",
    "def batched(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield i, iterable[i:i+n]\n",
    "\n",
    "all_topics = []\n",
    "for i, batch in tqdm(\n",
    "    batched(df_full[\"clean_for_topics\"].tolist(), BATCH_SIZE),\n",
    "    total=math.ceil(len(df_full) / BATCH_SIZE),\n",
    "    desc=\"Transform full\"\n",
    "):\n",
    "    bt, _ = topic_model.transform(batch)\n",
    "    all_topics.extend(bt)\n",
    "\n",
    "#Save per-doc results\n",
    "base_cols = [\"id\", \"subreddit\", \"kind\", \"dt\", \"clean_for_topics\"]\n",
    "if \"parent_topic\" in df_full.columns:\n",
    "    base_cols += [\"parent_topic\", \"parent_topic_name\"]\n",
    "\n",
    "out = df_full[base_cols].copy()\n",
    "out[\"topic\"] = all_topics\n",
    "name_map = topic_info.set_index(\"topic\")[\"name\"].to_dict()\n",
    "out[\"topic_name\"] = out[\"topic\"].map(name_map)\n",
    "out.to_csv(TOPICS_PER_DOC_CSV, index=False)\n",
    "print(f\"[ok] Wrote per-doc topics → {TOPICS_PER_DOC_CSV}\")\n",
    "\n",
    "\n",
    "# Small quality check\n",
    "print(out[\"topic\"].value_counts().head(10).sort_index())\n",
    "print(\"\\nPreview:\")\n",
    "display(out.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0086380-38a6-4b9b-87a9-ad90b1cebf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
