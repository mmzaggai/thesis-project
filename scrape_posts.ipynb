{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f060f8f-1a46-4ed2-8c56-8ab1be44d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 — SCRAPE POSTS\n",
    "# Outputs: ip_posts.csv\n",
    "\n",
    "# Inspired by https://github.com/fferegrino/r-worldnews-live-threads-ukraine/blob/main/download_threads.ipynb\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# Reddit API keys\n",
    "CLIENT_ID     = \"...\"        \n",
    "CLIENT_SECRET = \"...\" \n",
    "USER_AGENT    = \"...\"\n",
    "\n",
    "# Date window (inclusive start, inclusive end)\n",
    "DATE_START = dt.date(2020, 1, 1)\n",
    "DATE_END   = dt.date.today()  # or a fixed date like dt.date(2025, 9, 1)\n",
    "\n",
    "# Subreddits & keywords\n",
    "SUBREDDITS = [\"IsraelPalestine\", \"worldnews\", \"Palestine\", \"Judaism\"]\n",
    "KEYWORDS = [\n",
    "    \"Israel\",\"Palestine\",\"Gaza\",\"West Bank\",\"Hamas\",\"IDF\",\n",
    "    \"ceasefire\",\"hostages\",\"Hezbollah\",\"settlements\",\"UNRWA\",\"rafah\"\n",
    "]\n",
    "\n",
    "# Output files \n",
    "OUT_CSV   = \"ip_posts.csv\"\n",
    "OUT_JSONL = \"ip_posts.jsonl\" \n",
    "\n",
    "# Politeness\n",
    "SLEEP_BETWEEN = 0.4  # seconds between API reads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c068679-ab24-47fe-b6ff-4b7bed165e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, csv\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, List, Iterable, Tuple\n",
    "\n",
    "import praw\n",
    "\n",
    "@dataclass\n",
    "class Row:\n",
    "    id: str\n",
    "    created_utc: int\n",
    "    subreddit: str\n",
    "    author: Optional[str]\n",
    "    title: Optional[str]\n",
    "    selftext: Optional[str]\n",
    "    score: Optional[int]\n",
    "    num_comments: Optional[int]\n",
    "    link_flair_text: Optional[str]\n",
    "    url: Optional[str]\n",
    "    permalink: Optional[str]\n",
    "\n",
    "FIELDS = [f.name for f in Row.__dataclass_fields__.values()]\n",
    "\n",
    "def to_epoch(d: dt.date, end_of_day=False) -> int:\n",
    "    if end_of_day:\n",
    "        ts = dt.datetime(d.year, d.month, d.day, 23, 59, 59, tzinfo=dt.timezone.utc)\n",
    "    else:\n",
    "        ts = dt.datetime(d.year, d.month, d.day, 0, 0, 0, tzinfo=dt.timezone.utc)\n",
    "    return int(ts.timestamp())\n",
    "\n",
    "EPOCH_START = to_epoch(DATE_START, end_of_day=False)\n",
    "EPOCH_END   = to_epoch(DATE_END,   end_of_day=True)\n",
    "\n",
    "def flair_keep(sr: str, flair: Optional[str]) -> bool:\n",
    "    # Paper-style filtering\n",
    "    if sr.lower() == \"worldnews\":\n",
    "        return (flair or \"\").strip() == \"Israel/Palestine\"\n",
    "    if sr.lower() == \"judaism\":\n",
    "        return (flair or \"\").strip() == \"Israel Megathread\"\n",
    "    return True\n",
    "\n",
    "def keyword_hit(title: str, selftext: str) -> bool:\n",
    "    blob = f\"{title or ''} {selftext or ''}\".lower()\n",
    "    return any(k.lower() in blob for k in KEYWORDS)\n",
    "\n",
    "def ensure_csv(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.DictWriter(f, fieldnames=FIELDS).writeheader()\n",
    "\n",
    "def append_csv(path: str, rows: Iterable[dict]):\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=FIELDS)\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def append_jsonl(path: str, rows: Iterable[dict]):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_seen_ids(path: str) -> set:\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        return set()\n",
    "    try:\n",
    "        return set(pd.read_csv(path, usecols=[\"id\"])[\"id\"].astype(str))\n",
    "    except Exception:\n",
    "        ids = set()\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for row in csv.DictReader(f):\n",
    "                ids.add(row[\"id\"])\n",
    "        return ids\n",
    "\n",
    "def init_reddit():\n",
    "    assert all([CLIENT_ID, CLIENT_SECRET, USER_AGENT]) and CLIENT_ID != \"YOUR_CLIENT_ID\", \\\n",
    "        \"Fill CLIENT_ID/CLIENT_SECRET/USER_AGENT in the Config cell.\"\n",
    "    return praw.Reddit(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, user_agent=USER_AGENT, check_for_async=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c586c18-2db5-48c8-bfce-97bc760c7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_month_window(sr_name: str, month_start: dt.date) -> List[Row]:\n",
    "    \"\"\"\n",
    "    Try time-bounded search via legacy cloudsearch for one month.\n",
    "    Fallback to scanning 'new' (recent) and 'top(all)' (historic) for that window.\n",
    "    \"\"\"\n",
    "    reddit = init_reddit()\n",
    "    rows: List[Row] = []\n",
    "\n",
    "    # month bounds\n",
    "    if month_start.month == 12:\n",
    "        month_end = dt.date(month_start.year + 1, 1, 1) - dt.timedelta(days=1)\n",
    "    else:\n",
    "        month_end = dt.date(month_start.year, month_start.month + 1, 1) - dt.timedelta(days=1)\n",
    "\n",
    "    # clamp to global\n",
    "    if month_start < DATE_START: month_start = DATE_START\n",
    "    if month_end   > DATE_END:   month_end   = DATE_END\n",
    "    if month_start > month_end:  return rows\n",
    "\n",
    "    t0 = to_epoch(month_start, end_of_day=False)\n",
    "    t1 = to_epoch(month_end,   end_of_day=True)\n",
    "\n",
    "    sr = reddit.subreddit(sr_name)\n",
    "\n",
    "    #  A) Try cloudsearch timestamp query ---\n",
    "    q = f\"timestamp:{t0}..{t1}\"\n",
    "    got_any = False\n",
    "    try:\n",
    "        for s in sr.search(q, sort=\"new\", syntax=\"cloudsearch\", limit=None):\n",
    "            cu = int(getattr(s, \"created_utc\", 0) or 0)\n",
    "            if cu < t0 or cu > t1:  # defensive\n",
    "                continue\n",
    "            flair = getattr(s, \"link_flair_text\", None)\n",
    "            if not flair_keep(sr_name, flair):\n",
    "                continue\n",
    "            title = getattr(s, \"title\", \"\") or \"\"\n",
    "            selftext = getattr(s, \"selftext\", \"\") or \"\"\n",
    "            if not keyword_hit(title, selftext):\n",
    "                continue\n",
    "            rows.append(Row(\n",
    "                id=s.id,\n",
    "                created_utc=cu,\n",
    "                subreddit=str(getattr(s, \"subreddit\", \"\")),\n",
    "                author=str(getattr(s, \"author\", \"\") or \"\") or None,\n",
    "                title=title,\n",
    "                selftext=selftext,\n",
    "                score=int(getattr(s, \"score\", 0) or 0),\n",
    "                num_comments=int(getattr(s, \"num_comments\", 0) or 0),\n",
    "                link_flair_text=flair,\n",
    "                url=getattr(s, \"url\", None),\n",
    "                permalink=getattr(s, \"permalink\", None),\n",
    "            ))\n",
    "            got_any = True\n",
    "        if got_any:\n",
    "            return rows\n",
    "    except Exception:\n",
    "        # cloudsearch often fails or returns nothing — fall back\n",
    "        pass\n",
    "\n",
    "    # B) Fallback 1: scroll 'new' until older than t0 (good for recent months) ---\n",
    "    try:\n",
    "        for s in sr.new(limit=None):\n",
    "            cu = int(getattr(s, \"created_utc\", 0) or 0)\n",
    "            if cu < t0:\n",
    "                break  # older than our month\n",
    "            if cu > t1:\n",
    "                continue  # too new\n",
    "            flair = getattr(s, \"link_flair_text\", None)\n",
    "            if not flair_keep(sr_name, flair):\n",
    "                continue\n",
    "            title = getattr(s, \"title\", \"\") or \"\"\n",
    "            selftext = getattr(s, \"selftext\", \"\") or \"\"\n",
    "            if not keyword_hit(title, selftext):\n",
    "                continue\n",
    "            rows.append(Row(\n",
    "                id=s.id,\n",
    "                created_utc=cu,\n",
    "                subreddit=str(getattr(s, \"subreddit\", \"\")),\n",
    "                author=str(getattr(s, \"author\", \"\") or \"\") or None,\n",
    "                title=title,\n",
    "                selftext=selftext,\n",
    "                score=int(getattr(s, \"score\", 0) or 0),\n",
    "                num_comments=int(getattr(s, \"num_comments\", 0) or 0),\n",
    "                link_flair_text=flair,\n",
    "                url=getattr(s, \"url\", None),\n",
    "                permalink=getattr(s, \"permalink\", None),\n",
    "            ))\n",
    "        if rows:\n",
    "            return rows\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # C) Fallback 2: scan 'top(\"all\")' and pick those in [t0, t1] (not exhaustive, but gets salient posts) ---\n",
    "    try:\n",
    "        for s in sr.top(time_filter=\"all\", limit=2000):  # adjust if needed\n",
    "            cu = int(getattr(s, \"created_utc\", 0) or 0)\n",
    "            if cu < t0:\n",
    "                # top(all) is not chronological; we can't break early safely\n",
    "                continue\n",
    "            if cu > t1:\n",
    "                continue\n",
    "            flair = getattr(s, \"link_flair_text\", None)\n",
    "            if not flair_keep(sr_name, flair):\n",
    "                continue\n",
    "            title = getattr(s, \"title\", \"\") or \"\"\n",
    "            selftext = getattr(s, \"selftext\", \"\") or \"\"\n",
    "            if not keyword_hit(title, selftext):\n",
    "                continue\n",
    "            rows.append(Row(\n",
    "                id=s.id,\n",
    "                created_utc=cu,\n",
    "                subreddit=str(getattr(s, \"subreddit\", \"\")),\n",
    "                author=str(getattr(s, \"author\", \"\") or \"\") or None,\n",
    "                title=title,\n",
    "                selftext=selftext,\n",
    "                score=int(getattr(s, \"score\", 0) or 0),\n",
    "                num_comments=int(getattr(s, \"num_comments\", 0) or 0),\n",
    "                link_flair_text=flair,\n",
    "                url=getattr(s, \"url\", None),\n",
    "                permalink=getattr(s, \"permalink\", None),\n",
    "            ))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a176408b-249a-496c-8d47-2b69e86dc06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[subreddit] r/IsraelPalestine\n",
      "  [2020-05] wrote 2 posts\n",
      "  [2020-07] wrote 2 posts\n",
      "  [2020-08] wrote 3 posts\n",
      "  [2020-09] wrote 3 posts\n",
      "  [2020-12] wrote 1 posts\n",
      "  [2021-01] wrote 1 posts\n",
      "  [2021-02] wrote 2 posts\n",
      "  [2021-03] wrote 2 posts\n",
      "  [2021-04] wrote 2 posts\n",
      "  [2021-05] wrote 38 posts\n",
      "  [2021-06] wrote 26 posts\n",
      "  [2021-07] wrote 13 posts\n",
      "  [2021-08] wrote 4 posts\n",
      "  [2021-09] wrote 3 posts\n",
      "  [2021-10] wrote 2 posts\n",
      "  [2021-11] wrote 2 posts\n",
      "  [2022-01] wrote 1 posts\n",
      "  [2022-02] wrote 2 posts\n",
      "  [2022-03] wrote 3 posts\n",
      "  [2022-04] wrote 2 posts\n",
      "  [2022-05] wrote 7 posts\n",
      "  [2022-06] wrote 4 posts\n",
      "  [2022-07] wrote 4 posts\n",
      "  [2022-08] wrote 3 posts\n",
      "  [2022-09] wrote 1 posts\n",
      "  [2022-10] wrote 1 posts\n",
      "  [2023-01] wrote 1 posts\n",
      "  [2023-02] wrote 1 posts\n",
      "  [2023-04] wrote 2 posts\n",
      "  [2023-05] wrote 3 posts\n",
      "  [2023-06] wrote 1 posts\n",
      "  [2023-07] wrote 2 posts\n",
      "  [2023-08] wrote 1 posts\n",
      "  [2023-09] wrote 1 posts\n",
      "  [2023-10] wrote 161 posts\n",
      "  [2023-11] wrote 98 posts\n",
      "  [2023-12] wrote 52 posts\n",
      "  [2024-01] wrote 26 posts\n",
      "  [2024-02] wrote 30 posts\n",
      "  [2024-03] wrote 23 posts\n",
      "  [2024-04] wrote 39 posts\n",
      "  [2024-05] wrote 26 posts\n",
      "  [2024-06] wrote 22 posts\n",
      "  [2024-07] wrote 17 posts\n",
      "  [2024-08] wrote 18 posts\n",
      "  [2024-09] wrote 19 posts\n",
      "  [2024-10] wrote 28 posts\n",
      "  [2024-11] wrote 19 posts\n",
      "  [2024-12] wrote 19 posts\n",
      "  [2025-01] wrote 21 posts\n",
      "  [2025-02] wrote 27 posts\n",
      "  [2025-03] wrote 22 posts\n",
      "  [2025-04] wrote 26 posts\n",
      "  [2025-05] wrote 33 posts\n",
      "  [2025-06] wrote 37 posts\n",
      "  [2025-07] wrote 98 posts\n",
      "  [2025-08] wrote 521 posts\n",
      "  [2025-09] wrote 325 posts\n",
      "[subreddit] r/worldnews\n",
      "  [2020-05] wrote 1 posts\n",
      "  [2021-05] wrote 4 posts\n",
      "  [2025-09] wrote 102 posts\n",
      "[subreddit] r/Palestine\n",
      "  [2021-05] wrote 19 posts\n",
      "  [2021-06] wrote 7 posts\n",
      "  [2021-07] wrote 1 posts\n",
      "  [2022-02] wrote 1 posts\n",
      "  [2022-12] wrote 1 posts\n",
      "  [2023-01] wrote 2 posts\n",
      "  [2023-04] wrote 1 posts\n",
      "  [2023-10] wrote 4 posts\n",
      "  [2023-11] wrote 7 posts\n",
      "  [2023-12] wrote 6 posts\n",
      "  [2024-01] wrote 6 posts\n",
      "  [2024-02] wrote 17 posts\n",
      "  [2024-03] wrote 19 posts\n",
      "  [2024-04] wrote 13 posts\n",
      "  [2024-05] wrote 8 posts\n",
      "  [2024-06] wrote 6 posts\n",
      "  [2024-07] wrote 10 posts\n",
      "  [2024-08] wrote 8 posts\n",
      "  [2024-09] wrote 16 posts\n",
      "  [2024-10] wrote 32 posts\n",
      "  [2024-11] wrote 25 posts\n",
      "  [2024-12] wrote 20 posts\n",
      "  [2025-01] wrote 25 posts\n",
      "  [2025-02] wrote 16 posts\n",
      "  [2025-03] wrote 22 posts\n",
      "  [2025-04] wrote 33 posts\n",
      "  [2025-05] wrote 33 posts\n",
      "  [2025-06] wrote 42 posts\n",
      "  [2025-07] wrote 39 posts\n",
      "  [2025-08] wrote 166 posts\n",
      "  [2025-09] wrote 483 posts\n",
      "[subreddit] r/Judaism\n",
      "  [2025-07] wrote 1 posts\n",
      "  [2025-08] wrote 4 posts\n",
      "  [2025-09] wrote 3 posts\n",
      "[done] total posts written: 3056\n"
     ]
    }
   ],
   "source": [
    "def run_official_only():\n",
    "    ensure_csv(OUT_CSV)\n",
    "    seen = load_seen_ids(OUT_CSV)\n",
    "\n",
    "    # build month starts from DATE_START → DATE_END\n",
    "    months = []\n",
    "    d = dt.date(DATE_START.year, DATE_START.month, 1)\n",
    "    end_month = dt.date(DATE_END.year, DATE_END.month, 1)\n",
    "    while d <= end_month:\n",
    "        months.append(d)\n",
    "        # advance a month\n",
    "        if d.month == 12:\n",
    "            d = dt.date(d.year + 1, 1, 1)\n",
    "        else:\n",
    "            d = dt.date(d.year, d.month + 1, 1)\n",
    "\n",
    "    total = 0\n",
    "    for sr in SUBREDDITS:\n",
    "        print(f\"[subreddit] r/{sr}\")\n",
    "        for m in months:\n",
    "            rows = search_month_window(sr, m)\n",
    "            if not rows:\n",
    "                continue\n",
    "            # dedupe against existing file + within this batch\n",
    "            deduped = []\n",
    "            for r in rows:\n",
    "                if r.id not in seen:\n",
    "                    deduped.append(asdict(r))\n",
    "                    seen.add(r.id)\n",
    "            if deduped:\n",
    "                append_csv(OUT_CSV, deduped)\n",
    "                if OUT_JSONL:\n",
    "                    append_jsonl(OUT_JSONL, deduped)\n",
    "                total += len(deduped)\n",
    "                print(f\"  [{m.strftime('%Y-%m')}] wrote {len(deduped)} posts\")\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "    print(f\"[done] total posts written: {total}\")\n",
    "\n",
    "run_official_only()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01859381-4a6b-40e6-9048-a077abb5a642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fe68d-6190-474c-9288-2a3042e1bc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
